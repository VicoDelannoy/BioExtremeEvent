---
title: "BEE.calc.metrics_point"
author: "Vico"
format: html
editor: visual
---

```{r}
# start_date <- "2022-01-01" ; end_date <- "2023-12-31" ; Values <- ds
BEE.calc.metrics_point <- function(Events_corrected, Values, GPS, start_date=NULL, end_date=NULL){
  ########################################## WARNINGS ############################################################
  if ( is.null(start_date) | is.null(end_date)) {
    warning("You didn't specify a begining date and a ending date (see argument 'start_date' and 'end_date'), the first date and last date in your time Values SpatRaster will be used.")
    start_date <- min(as.Date.character(names(Values)))
    end_date <- max(as.Date.character(names(period_of_interest)))
  }
  if(class(start_date) != class(names(Values[[1]]))  | 
     class(end_date)   != class(names(Values[[1]]))  | 
     class(start_date) != class(end_date)){
    warning("The date formats are inconsistent between start_date, end_date and Values. Please ensure that the layer names follow a consistent date format. Use class(YourObject) to verify the current format.")
  }
  #Check that start date and end_date are within the SpatRasters provided
  if (!(start_date %in% names(Values)) | !(end_date %in% names(Values))) {
    warning("One or both the specified layers are not present in the SpatRaster containning corrected binarized extreme event.")
  }
  Values_extent <- terra::ext(Values)
  #Check that all GPS points are within Values and period_of_interest extent
  if ( !all(abs(GPS[1]) <= abs(Values_extent$xmax) & 
            abs(GPS[1]) >= abs(Values_extent$xmin)) ){
   warning("At least one longitude coordinate falls outside the extent of the SpatRaster containing the binarized corrected events. Ensure the first column of the dataframe contains valid longitude (x) values for analysis.")
  }
  if (!all(abs(GPS[2]) <= abs(Values_extent$ymax) & 
           abs(GPS[2]) >= abs(Values_extent$ymin))){
   warning("At least one latitude coordinate falls outside the extent of the SpatRaster containing the binarized corrected events. Ensure the second column of the dataframe contains valid latitude (y) values for analysis.")
  }

   ########################################## CODE ############################################################
    #Extract values for the given GPS position 
  GPS$pixel <- terra::cellFromXY(Values, GPS)
  df_list  <- lapply(GPS$pixel, function(p) Events_corrected[[p]])
  Values <- t(terra::extract(Values,GPS[,3]))
    
    # Subset both dataset so they match the timeframe provided with 'start_date' and 'end_date'.
  Date <- rownames(Values)
  Values <- Values[which(as.Date(rownames(Values)) >= as.Date(start_date) & as.Date(rownames(Values)) <= as.Date(end_date)),]
  df_list <- Map(function(df, col_idx){
    df$Date <- Date
    return(df)
  }, df_list, seq_along(df_list))
  df_list <- map(df_list, ~ .x[.x$Date >= start_date & .x$Date <= end_date, ])
  # For each event I want : duration, maximum intensity, mean and median intensity, category, sum of anomalies, date of maximum intensity, position of the day of maximum intensity, mean increasing slop, mean decreasing slope, start_date, end_date
  ##Contrary to Events_corrected, Values didn't kept this information of which extreme event were merge together because they were separated by d days or less. Thus, we need to use Events corrected if we want to calculate a metric that describe an event.
  
  #Merge df_list and Values
  df_list <- Map(function(df, col_idx) {
    # Ad to each dataframe the corresponding column of pixel value
    df$value <- Values[,col_idx]  # col_idx is he idee  
    return(df)
  }, df_list, seq_along(df_list))
  
  #Delete the information related to days bellow threshold (to save computation time)
  event_to_delete_list <- lapply(df_list, function(df) {
  unique(df[which(df$Cleanned_value == 0),"ID"])})
  df_list_only_1 <- lapply(seq_along(df_list), function(p) { # we need to save the data that correspond to value above threshold in a specific object beacause we may want to optimize the function so it test the onset rate before the event start (and thus we will need values bellow threshold as well)
    df_list[[p]] %>%
    filter(!(ID %in% event_to_delete_list[[p]]))
  })
  
  #Create a list of dataframe to store the information using one element (df) per pixel and one row per event
  colnames(GPS)<-c("x","y", names(GPS[3]))

  metrics <- lapply(1:length(df_list_only_1), function(p) { # Go through each pixel
    df <- df_list_only_1[[p]]
    df$Date <- as.Date(df$Date)
    df <- df %>%
      group_by(ID) %>%
      mutate(
        # Get GPS position of each point
        x = GPS$x[p],  
        y = GPS$y[p],  
    
        # Get the ID of each event
        event_ID = unique(ID),  
    
        # Duration of each event (assuming Nb_days is constant per ID)
        Nb_days = first(Nb_days),  
    
        # First and last day of each event
        first_date = as.Date(min(Date)),
        last_date = as.Date(max(Date)),
    
        # Mean, median, max temperature per event
        mean_value = mean(value),
        median_value = median(value),
        max_value = max(value),
    
        # Day of maximum temperature
        date_max_value = as.Date(Date[which.max(value)]),
    
        # Onset rate
        daily_rates = (lead(value) - value) / as.numeric(lead(Date) - Date),
        ## raw
        days_onset = as.numeric(date_max_value - first_date),
        raw_onset_rate = (max_value - value[which(Date==first_date)]) / days_onset,
        ## mean
        mean_onset_rate = mean(daily_rates[Date >= first_date & Date <= date_max_value], na.rm = TRUE),
        ## Median onset rate 
        median_onset_rate = median(daily_rates[Date >= first_date & Date <= date_max_value], na.rm = TRUE),
        ## Standard deviation
        sd_onset_rate = sd(daily_rates[Date >= first_date & Date <= date_max_value], na.rm = TRUE),
    
        # Offset rate
        ## raw
        days_offset = as.numeric(last_date - date_max_value),
        raw_offset_rate = (value[which(Date==last_date)] - max_value) / days_offset,
        ## mean
        mean_offset_rate = mean(daily_rates[Date >= date_max_value & Date <= last_date], na.rm = TRUE),
        median_offset_rate = median(daily_rates[Date >= date_max_value & Date <= last_date], na.rm = TRUE),
        ## Standard deviation
        sd_offset_rate = sd(daily_rates[Date >= date_max_value & Date <= last_date], na.rm = TRUE)

      ) %>%
      ungroup()  # Pour éviter tout effet résiduel du groupement
    return(df)
  })

  # OTHER PART TO DEVELOP : 
  # For each pixels : 
  # - anomalie cumulée par événements
  # - catégories
  # - onset rate, offset_rate (dans l'événement et depuis une période donnée avant et après dont la durée et fixée par l'utilisateur)
  #For the all period I want : Frequency, maximum intensity, mean and median intensity, sum of anomalies, date of maximum intensity, first date 1, last date 1
}
```
